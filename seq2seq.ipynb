{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附加练习作业（选做，折平时分）  seq2seq翻译改进练习"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "为了提升效果，本示例程序将英文原按字符分割改进为按单词分割。\n",
    "请在理解seq2seq代码基础上，完成代码改造工作，提高性能。要求：\n",
    "1）文本分割处理：注意简单空格分割会导致标点符号没有分离，标点符号有意义应作为单独token。非标点符号删除（isn't的引号如何处理可斟酌）\n",
    "2）模型结构：改为GRU单元，多层堆砌，参数优化：神经元数量 等优化  \n",
    "3）训练过程保存最佳模型用于预测  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 25 19:21:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "| 41%   45C    P8    33W / 350W |  10247MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:03:00.0 Off |                  N/A |\n",
      "| 65%   57C    P2   134W / 350W |  17843MiB / 24268MiB |     11%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 30%   34C    P8    18W / 350W |   5866MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3552      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    591684      C   /data/anaconda3/bin/python       1583MiB |\n",
      "|    0   N/A  N/A    622119      C   /data/anaconda3/bin/python       2095MiB |\n",
      "|    0   N/A  N/A    630044      C   /data/anaconda3/bin/python       3679MiB |\n",
      "|    0   N/A  N/A    746426      C   /data/anaconda3/bin/python       1441MiB |\n",
      "|    0   N/A  N/A    816682      C   /data/anaconda3/bin/python       1441MiB |\n",
      "|    1   N/A  N/A      3552      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A    589422      C   /data/anaconda3/bin/python       3679MiB |\n",
      "|    1   N/A  N/A    591528      C   /data/anaconda3/bin/python       4703MiB |\n",
      "|    1   N/A  N/A    615304      C   /data/anaconda3/bin/python       3119MiB |\n",
      "|    1   N/A  N/A    623767      C   /data/anaconda3/bin/python       2655MiB |\n",
      "|    1   N/A  N/A    813161      C   /data/anaconda3/bin/python       3679MiB |\n",
      "|    2   N/A  N/A      3552      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    2   N/A  N/A    595085      C   /data/anaconda3/bin/python       1441MiB |\n",
      "|    2   N/A  N/A    596117      C   /data/anaconda3/bin/python       4417MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "#首先执行GPU资源分配代码，勿删除。\n",
    "import GPU\n",
    "GPU.show()\n",
    "GPU.alloc(0,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,Embedding,GRU\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNITS = 256\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 50\n",
    "NUM_SAMPLES = 21000\n",
    "embedding_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(data_path,header=None).iloc[:NUM_SAMPLES,:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>I listen to the radio after dinner.</td>\n",
       "      <td>晚饭后我听收音机。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>I live in an apartment in the city.</td>\n",
       "      <td>我住在城市中的一間公寓裡。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>I look forward to hearing from you.</td>\n",
       "      <td>我期待收到你的來信。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>I look forward to seeing you again.</td>\n",
       "      <td>我期待著再次見到你。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>I lost my ticket. What should I do?</td>\n",
       "      <td>我丢了我的票。我該怎麼辦？</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0              1\n",
       "20995  I listen to the radio after dinner.      晚饭后我听收音机。\n",
       "20996  I live in an apartment in the city.  我住在城市中的一間公寓裡。\n",
       "20997  I look forward to hearing from you.     我期待收到你的來信。\n",
       "20998  I look forward to seeing you again.     我期待著再次見到你。\n",
       "20999  I lost my ticket. What should I do?  我丢了我的票。我該怎麼辦？"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\t嗨。\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\t你好。\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run.</td>\n",
       "      <td>\\t你用跑的。\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop!</td>\n",
       "      <td>\\t住手！\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wait!</td>\n",
       "      <td>\\t等等！\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  inputs    targets\n",
       "0    Hi.     \\t嗨。\\n\n",
       "1    Hi.    \\t你好。\\n\n",
       "2   Run.  \\t你用跑的。\\n\n",
       "3  Stop!    \\t住手！\\n\n",
       "4  Wait!    \\t等等！\\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=['inputs','targets']\n",
    "df['targets'] = df['targets'].apply(lambda x: '\\t'+x+'\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df.inputs.values.tolist()\n",
    "target_texts = df.targets.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理英文句子\n",
    "def preprocess_sentence(sentence):\n",
    "    # 添加空格以分隔符号\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    # 将非字母和非标点符号的字符替换为空格\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    # 去除多余的空格\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "input_texts=[ preprocess_sentence(t) for t in input_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4,加载数据;使用tokenizer做词嵌入;分割数据为train,vaild,test数据集\n",
    "def max_length(tensor):\n",
    "    \"\"\"找到数据集中padding的最大值\"\"\"\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang, china = False):\n",
    "    \"\"\"将数据集做padding\"\"\"\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level = china)\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts, china=True)\n",
    "max_length_targ= max_length(target_tensor)\n",
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PADDING字符占用一个码，需要+1\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_inp =  max_length(input_tensor)\n",
    "max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> i\n",
      "174 ----> lost\n",
      "14 ----> my\n",
      "670 ----> ticket\n",
      "1 ----> .\n",
      "23 ----> what\n",
      "79 ----> should\n",
      "2 ----> i\n",
      "15 ----> do\n",
      "4 ----> ?\n"
     ]
    }
   ],
   "source": [
    "# 6,测试数据转化结果 \n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang_tokenizer, input_tensor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Language; index to word mapping\n",
      "1 ----> \t\n",
      "4 ----> 我\n",
      "673 ----> 丢\n",
      "7 ----> 了\n",
      "4 ----> 我\n",
      "5 ----> 的\n",
      "437 ----> 票\n",
      "3 ----> 。\n",
      "4 ----> 我\n",
      "172 ----> 該\n",
      "93 ----> 怎\n",
      "43 ----> 麼\n",
      "648 ----> 辦\n",
      "9 ----> ？\n",
      "2 ----> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang_tokenizer, target_tensor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 20000 1000 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 5,拆分训练集和验证集\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=1000)\n",
    "\n",
    "# 打印数据集长度 - Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder的label tensor\n",
    "output_tensor_train=np.zeros_like(target_tensor_train)\n",
    "output_tensor_train[:,:-1]=target_tensor_train[:,1:]\n",
    "\n",
    "output_tensor_val=np.zeros_like(target_tensor_val)\n",
    "output_tensor_val[:,:-1]=target_tensor_val[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21000, 12), (21000, 22))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型（作业要点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(n_input,n_output,n_units):\n",
    "#     #训练阶段\n",
    "#     #encoder\n",
    "#     encoder_input = Input(shape = (None, ))\n",
    "#     embeddin = keras.layers.Embedding(n_input, embedding_dim)\n",
    "#     #encoder输入维度n_input为每个时间步的输入xt的维度，这里是用来one-hot的英文字符数\n",
    "#     encoder = LSTM(n_units, return_state=True)\n",
    "#     #n_units为LSTM单元中每个门的神经元的个数，return_state设为True时才会返回最后时刻的状态h,c\n",
    "#     _,encoder_h,encoder_c = encoder(embeddin(encoder_input))\n",
    "#     encoder_state = [encoder_h,encoder_c]\n",
    "#     #保留下来encoder的末状态作为decoder的初始状态\n",
    "    \n",
    "#     #decoder\n",
    "#     decoder_input = Input(shape = (None, ))\n",
    "#     embeddout = keras.layers.Embedding(n_output, embedding_dim)\n",
    "#     #decoder的输入维度为中文字符数\n",
    "#     decoder = LSTM(n_units,return_sequences=True, return_state=True)\n",
    "#     #训练模型时需要decoder的输出序列来与结果对比优化，故return_sequences也要设为True\n",
    "#     decoder_output, _, _ = decoder(embeddout(decoder_input),initial_state=encoder_state)\n",
    "#     #在训练阶段只需要用到decoder的输出序列，不需要用最终状态h.c\n",
    "#     decoder_dense = Dense(n_output,activation='softmax')\n",
    "#     decoder_output = decoder_dense(decoder_output)\n",
    "#     #输出序列经过全连接层得到结果\n",
    "    \n",
    "#     #生成的训练模型\n",
    "#     model = Model([encoder_input,decoder_input],decoder_output)\n",
    "#     #第一个参数为训练模型的输入，包含了encoder和decoder的输入，第二个参数为模型的输出，包含了decoder的输出\n",
    "    \n",
    "#     #推理阶段，用于预测过程\n",
    "#     #推断模型—encoder\n",
    "#     encoder_infer = Model(encoder_input,encoder_state)\n",
    "    \n",
    "#     #推断模型-decoder\n",
    "#     decoder_state_input_h = Input(shape=(n_units,))\n",
    "#     decoder_state_input_c = Input(shape=(n_units,))    \n",
    "#     decoder_state_input = [decoder_state_input_h, decoder_state_input_c]#上个时刻的状态h,c   \n",
    "    \n",
    "#     decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(embeddout(decoder_input),initial_state=decoder_state_input)\n",
    "#     decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]#当前时刻得到的状态\n",
    "#     decoder_infer_output = decoder_dense(decoder_infer_output)#当前时刻的输出\n",
    "#     decoder_infer = Model([decoder_input]+decoder_state_input,[decoder_infer_output]+decoder_infer_state)\n",
    "    \n",
    "#     return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(n_input, n_output, n_units):\n",
    "#     # 训练阶段\n",
    "#     # Encoder\n",
    "#     encoder_input = Input(shape=(None,))\n",
    "#     embedding_dim = 64  # 假设embedding维度为64\n",
    "#     embedding = Embedding(n_input, embedding_dim)\n",
    "#     encoder = GRU(n_units, return_sequences=False, return_state=True)  # 使用GRU替换LSTM\n",
    "#     encoder_output, encoder_state = encoder(embedding(encoder_input))\n",
    "    \n",
    "#     # Decoder\n",
    "#     decoder_input = Input(shape=(None,))\n",
    "#     decoder_embedding = Embedding(n_output, embedding_dim)\n",
    "#     decoder = GRU(n_units, return_sequences=True, return_state=True)  # 使用GRU替换LSTM\n",
    "#     decoder_output, _ = decoder(decoder_embedding(decoder_input), initial_state=encoder_state)\n",
    "    \n",
    "#     decoder_dense = Dense(n_output, activation='softmax')\n",
    "#     decoder_output = decoder_dense(decoder_output)\n",
    "    \n",
    "#     # 生成训练模型\n",
    "#     model = Model([encoder_input, decoder_input], decoder_output)\n",
    "    \n",
    "#     # 推理阶段，用于预测过程\n",
    "#     # 推断模型 - Encoder\n",
    "#     encoder_infer = Model(encoder_input, encoder_state)\n",
    "    \n",
    "#     # 推断模型 - Decoder\n",
    "#     decoder_state_input = Input(shape=(n_units,))\n",
    "#     decoder_infer_output, decoder_infer_state = decoder(decoder_embedding(decoder_input), initial_state=decoder_state_input)\n",
    "#     decoder_infer_output = decoder_dense(decoder_infer_output)\n",
    "    \n",
    "#     decoder_infer = Model([decoder_input, decoder_state_input], [decoder_infer_output, decoder_infer_state])\n",
    "    \n",
    "#     return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input, n_output, n_units):\n",
    "    # 训练阶段\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(None,))\n",
    "    embedding_dim = 256\n",
    "    embedding = Embedding(n_input, embedding_dim)\n",
    "    encoder = GRU(n_units, return_sequences=False, return_state=True)\n",
    "    encoder_output, encoder_state = encoder(embedding(encoder_input))\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(None,))\n",
    "    decoder_embedding = Embedding(n_output, embedding_dim)\n",
    "    decoder = GRU(n_units, return_sequences=True, return_state=True)\n",
    "    decoder_output, _ = decoder(decoder_embedding(decoder_input), initial_state=encoder_state)\n",
    "    \n",
    "    decoder_dense = Dense(n_output, activation='softmax')\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "    \n",
    "    # 生成训练模型\n",
    "    model = Model([encoder_input, decoder_input], decoder_output)\n",
    "    \n",
    "    # 推理阶段，用于预测过程\n",
    "    # 推断模型 - Encoder\n",
    "    encoder_infer = Model(encoder_input, encoder_state)\n",
    "    \n",
    "    # 推断模型 - Decoder\n",
    "    decoder_state_input = Input(shape=(n_units,))\n",
    "    decoder_infer_output, decoder_infer_state = decoder(decoder_embedding(decoder_input), initial_state=decoder_state_input)\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)\n",
    "    \n",
    "    decoder_infer = Model([decoder_input, decoder_state_input], [decoder_infer_output, decoder_infer_state])\n",
    "    \n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train, encoder_infer, decoder_infer = create_model(vocab_inp_size, vocab_tar_size, N_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tensor, target_tensor\n",
    "\n",
    "model_train.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         1299200   \n",
      "                                                                 \n",
      " gru (GRU)                   [(None, 256),             394752    \n",
      "                              (None, 256)]                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,693,952\n",
      "Trainable params: 1,693,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_infer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 [==============================] - 11s 16ms/step - loss: 2.1512 - val_loss: 1.8062\n",
      "Epoch 2/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.6675 - val_loss: 1.5900\n",
      "Epoch 3/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.4885 - val_loss: 1.4772\n",
      "Epoch 4/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.3702 - val_loss: 1.3932\n",
      "Epoch 5/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.2766 - val_loss: 1.3362\n",
      "Epoch 6/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.1961 - val_loss: 1.2876\n",
      "Epoch 7/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.1233 - val_loss: 1.2408\n",
      "Epoch 8/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 1.0572 - val_loss: 1.2080\n",
      "Epoch 9/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.9963 - val_loss: 1.1819\n",
      "Epoch 10/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.9411 - val_loss: 1.1561\n",
      "Epoch 11/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.8890 - val_loss: 1.1383\n",
      "Epoch 12/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.8421 - val_loss: 1.1220\n",
      "Epoch 13/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.7972 - val_loss: 1.1161\n",
      "Epoch 14/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.7558 - val_loss: 1.1025\n",
      "Epoch 15/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.7160 - val_loss: 1.0965\n",
      "Epoch 16/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6791 - val_loss: 1.0931\n",
      "Epoch 17/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6436 - val_loss: 1.0945\n",
      "Epoch 18/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6106 - val_loss: 1.0916\n",
      "Epoch 19/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5798 - val_loss: 1.0943\n",
      "Epoch 20/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5509 - val_loss: 1.1000\n",
      "Epoch 21/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5238 - val_loss: 1.1051\n",
      "Epoch 22/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4977 - val_loss: 1.1042\n",
      "Epoch 23/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4739 - val_loss: 1.1116\n",
      "Epoch 24/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4504 - val_loss: 1.1172\n",
      "Epoch 25/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4285 - val_loss: 1.1174\n",
      "Epoch 26/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.4072 - val_loss: 1.1222\n",
      "Epoch 27/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3875 - val_loss: 1.1329\n",
      "Epoch 28/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3688 - val_loss: 1.1375\n",
      "Epoch 29/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3506 - val_loss: 1.1447\n",
      "Epoch 30/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3326 - val_loss: 1.1537\n",
      "Epoch 31/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3169 - val_loss: 1.1612\n",
      "Epoch 32/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3023 - val_loss: 1.1690\n",
      "Epoch 33/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2878 - val_loss: 1.1784\n",
      "Epoch 34/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2748 - val_loss: 1.1862\n",
      "Epoch 35/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2618 - val_loss: 1.1867\n",
      "Epoch 36/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2500 - val_loss: 1.1987\n",
      "Epoch 37/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2382 - val_loss: 1.1998\n",
      "Epoch 38/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2268 - val_loss: 1.2120\n",
      "Epoch 39/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2170 - val_loss: 1.2254\n",
      "Epoch 40/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.2076 - val_loss: 1.2228\n",
      "Epoch 41/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1989 - val_loss: 1.2301\n",
      "Epoch 42/50\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.1898 - val_loss: 1.2395\n",
      "Epoch 43/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1818 - val_loss: 1.2465\n",
      "Epoch 44/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1749 - val_loss: 1.2496\n",
      "Epoch 45/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1681 - val_loss: 1.2619\n",
      "Epoch 46/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1615 - val_loss: 1.2609\n",
      "Epoch 47/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1558 - val_loss: 1.2685\n",
      "Epoch 48/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1498 - val_loss: 1.2691\n",
      "Epoch 49/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1442 - val_loss: 1.2767\n",
      "Epoch 50/50\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.1390 - val_loss: 1.2833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa030da21d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit([input_tensor_train, target_tensor_train],output_tensor_train,batch_size=BATCH_SIZE,epochs=EPOCH,validation_data=([input_tensor_val, target_tensor_val],output_tensor_val) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source,encoder_inference, decoder_inference, n_steps, features):\n",
    "    #先通过推理encoder获得预测输入序列的隐状态\n",
    "    state = encoder_inference.predict(source)\n",
    "    #第一个字符'\\t',为起始标志\n",
    "    predict_seq = np.zeros((1,1))\n",
    "    predict_seq[0,0]=1   ##target_dict['\\t']\n",
    "\n",
    "    output = ''\n",
    "    #开始对encoder获得的隐状态进行推理\n",
    "    #每次循环用上次预测的字符作为输入来预测下一次的字符，直到预测出了终止符\n",
    "    for i in range(n_steps):#n_steps为句子最大长度\n",
    "        #给decoder输入上一个时刻的h,c隐状态，以及上一次的预测字符predict_seq\n",
    "        yhat,h = decoder_inference.predict([predict_seq]+[state])\n",
    "        #注意，这里的yhat为Dense之后输出的结果，因此与h不同\n",
    "        char_index = np.argmax(yhat[0,-1,:])\n",
    "        char = targ_lang_tokenizer.index_word[char_index]\n",
    "        output += char\n",
    "        state = h#本次状态做为下一次的初始状态继续传递\n",
    "        predict_seq = np.zeros((1,1))\n",
    "        predict_seq[0,0]=char_index\n",
    "        if char == '\\n':#预测到了终止符则停下来\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertinp(tensor):\n",
    "    s=''\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            s += inp_lang_tokenizer.index_word[t]+' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are going too far . \n",
      "你的太多了第一起。\n",
      "\n",
      "my grandfather lived a long life . \n",
      "我的祖父是活费了。\n",
      "\n",
      "whose is this car ? \n",
      "這把傘是誰嗎?\n",
      "\n",
      "i can t resist sweet things . \n",
      "我不能有一些錢。\n",
      "\n",
      "it wasn t interesting at all . \n",
      "它根本不是很有用。\n",
      "\n",
      "i guess it depends on the weather . \n",
      "我想知道這個派對。\n",
      "\n",
      "the stripes were horizontal . \n",
      "這個小男孩把所有把書被開掉。\n",
      "\n",
      "my father will kill me . \n",
      "我爸会杀了我的。\n",
      "\n",
      "turn off the tv . \n",
      "把電視關了。\n",
      "\n",
      "lie on your right side . \n",
      "警方不應該收取。\n",
      "\n",
      "he is very fond of music . \n",
      "他非常喜欢音乐。\n",
      "\n",
      "i was very tired last night . \n",
      "昨晚我感到有很多以前打了。\n",
      "\n",
      "we have many goals . \n",
      "我們有用他的。\n",
      "\n",
      "i am too short . \n",
      "我是對的。\n",
      "\n",
      "i guess that would be all right . \n",
      "我猜那應該可以的。\n",
      "\n",
      "my passport was stolen . \n",
      "我护照给偷了。\n",
      "\n",
      "one plus two equals three . \n",
      "有三个姐姐。\n",
      "\n",
      "we know that . \n",
      "我们知道。\n",
      "\n",
      "this tie doesn t go with my suit . \n",
      "这个男孩，我不在乎是否新會回來。\n",
      "\n",
      "sorry , i don t have any change . \n",
      "不好意思，我没能做过。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    test = input_tensor_val[i:i+1,:]#i:i+1保持数组是三维\n",
    "    out = predict_chinese(test,encoder_infer,decoder_infer,max_length_targ,vocab_tar_size)\n",
    "\n",
    "    print(convertinp(input_tensor_val[i]))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
